{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load the Boston dataset.\"\"\"\n",
    "\n",
    "    boston = datasets.load_boston()\n",
    "    return boston\n",
    "\n",
    "def explore_city_data(city_data):\n",
    "    \"\"\"Calculate the Boston housing statistics.\"\"\"\n",
    "\n",
    "    # Get the labels and features from the housing data\n",
    "    housing_prices = city_data.target\n",
    "    housing_features = city_data.data\n",
    "\n",
    "def split_data(city_data):\n",
    "    \"\"\"Randomly shuffle the sample set. Divide it into 70 percent training and 30 percent testing data.\"\"\"\n",
    "\n",
    "    # Get the features and labels from the Boston housing data\n",
    "    X, y = city_data.data, city_data.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def performance_metric(label, prediction):\n",
    "    \"\"\"Calculate and return the appropriate error performance metric.\"\"\"\n",
    "\n",
    "    # The following page has a table of scoring functions in sklearn:\n",
    "    # http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics\n",
    "    # Mean squared error seems pretty good for a regression problem\n",
    "    return mean_squared_error(label, prediction)\n",
    "\n",
    "    pass\n",
    "\n",
    "def learning_curve(depth, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Calculate the performance of the model after a set of training data.\"\"\"\n",
    "\n",
    "    # We will vary the training set size so that we have 50 different sizes\n",
    "    sizes = np.round(np.linspace(1, len(X_train), 50))\n",
    "    train_err = np.zeros(len(sizes))\n",
    "    test_err = np.zeros(len(sizes))\n",
    "\n",
    "    print (\"Decision Tree with Max Depth: \")\n",
    "    print (depth)\n",
    "\n",
    "    for i, s in enumerate(sizes):\n",
    "\n",
    "        # Create and fit the decision tree regressor model\n",
    "        regressor = DecisionTreeRegressor(max_depth=depth)\n",
    "        regressor.fit(X_train[:s], y_train[:s])\n",
    "\n",
    "        # Find the performance on the training and testing set\n",
    "        train_err[i] = performance_metric(y_train[:s], regressor.predict(X_train[:s]))\n",
    "        test_err[i] = performance_metric(y_test, regressor.predict(X_test))\n",
    "\n",
    "    # Plot learning curve graph\n",
    "    learning_curve_graph(sizes, train_err, test_err)\n",
    "\n",
    "def learning_curve_graph(sizes, train_err, test_err):\n",
    "    \"\"\"Plot training and test error as a function of the training size.\"\"\"\n",
    "\n",
    "    pl.figure()\n",
    "    pl.title('Decision Trees: Performance vs Training Size')\n",
    "    pl.plot(sizes, test_err, lw=2, label = 'test error')\n",
    "    pl.plot(sizes, train_err, lw=2, label = 'training error')\n",
    "    pl.legend()\n",
    "    pl.xlabel('Training Size')\n",
    "    pl.ylabel('Error')\n",
    "    pl.show()\n",
    "\n",
    "def model_complexity(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Calculate the performance of the model as model complexity increases.\"\"\"\n",
    "\n",
    "    print (\"Model Complexity: \")\n",
    "\n",
    "    # We will vary the depth of decision trees from 2 to 25\n",
    "    max_depth = np.arange(1, 25)\n",
    "    train_err = np.zeros(len(max_depth))\n",
    "    test_err = np.zeros(len(max_depth))\n",
    "\n",
    "    for i, d in enumerate(max_depth):\n",
    "        # Setup a Decision Tree Regressor so that it learns a tree with depth d\n",
    "        regressor = DecisionTreeRegressor(max_depth=d)\n",
    "\n",
    "        # Fit the learner to the training data\n",
    "        regressor.fit(X_train, y_train)\n",
    "\n",
    "        # Find the performance on the training set\n",
    "        train_err[i] = performance_metric(y_train, regressor.predict(X_train))\n",
    "\n",
    "        # Find the performance on the testing set\n",
    "        test_err[i] = performance_metric(y_test, regressor.predict(X_test))\n",
    "\n",
    "    # Plot the model complexity graph\n",
    "    model_complexity_graph(max_depth, train_err, test_err)\n",
    "\n",
    "def model_complexity_graph(max_depth, train_err, test_err):\n",
    "    \"\"\"Plot training and test error as a function of the depth of the decision tree learn.\"\"\"\n",
    "\n",
    "    pl.figure()\n",
    "    pl.title('Decision Trees: Performance vs Max Depth')\n",
    "    pl.plot(max_depth, test_err, lw=2, label = 'test error')\n",
    "    pl.plot(max_depth, train_err, lw=2, label = 'training error')\n",
    "    pl.legend()\n",
    "    pl.xlabel('Max Depth')\n",
    "    pl.ylabel('Error')\n",
    "    pl.show()\n",
    "\n",
    "\n",
    "def fit_predict_model(city_data):\n",
    "    \"\"\"Find and tune the optimal model. Make a prediction on housing data.\"\"\"\n",
    "\n",
    "    # Get the features and labels from the Boston housing data\n",
    "    X, y = city_data.data, city_data.target\n",
    "\n",
    "    # Setup a Decision Tree Regressor\n",
    "    reg = DecisionTreeRegressor()\n",
    "\n",
    "    parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}\n",
    "\n",
    "    #mean_squared_error(label, prediction)\n",
    "\n",
    "    # 1. Find an appropriate performance metric. This should be the same as the\n",
    "    # one used in your performance_metric procedure above:\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n",
    "\n",
    "    scorer = make_scorer(mean_squared_error)\n",
    "\n",
    "    # 2. We will use grid search to fine tune the Decision Tree Regressor and\n",
    "    # obtain the parameters that generate the best training performance. Set up\n",
    "    # the grid search object here.\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV\n",
    "\n",
    "    reg = GridSearchCV(DecisionTreeRegressor(), parameters, scoring=scorer)\n",
    "\n",
    "    # This has been added in to see the bias versus variance tradeoff as depth increases\n",
    "    print(reg.grid_scores_)\n",
    "\n",
    "    # Fit the learner to the training data to obtain the best parameter set\n",
    "    print (\"Final Model: \")\n",
    "    print (reg.fit(X, y))\n",
    "\n",
    "    # Use the model to predict the output of a particular sample\n",
    "    x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]\n",
    "    y = reg.predict(x)\n",
    "    print (\"House: \" + str(x))\n",
    "    print (\"Prediction: \" + str(y))\n",
    "\n",
    "#In the case of the documentation page for GridSearchCV, it might be the case that the example is just a demonstration of syntax for use of the function, rather than a statement about\n",
    "def main():\n",
    "    \"\"\"Analyze the Boston housing data. Evaluate and validate the\n",
    "    performanance of a Decision Tree regressor on the housing data.\n",
    "    Fine tune the model to make prediction on unseen data.\"\"\"\n",
    "\n",
    "    # Load data\n",
    "    city_data = load_data()\n",
    "\n",
    "    # Explore the data\n",
    "    explore_city_data(city_data)\n",
    "\n",
    "    # Training/Test dataset split\n",
    "    X_train, y_train, X_test, y_test = split_data(city_data)\n",
    "\n",
    "    # Learning Curve Graphs\n",
    "    max_depths = [1,2,3,4,5,6,7,8,9,10]\n",
    "    for max_depth in max_depths:\n",
    "        learning_curve(max_depth, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Model Complexity Graph\n",
    "    model_complexity(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Tune and predict Model\n",
    "    fit_predict_model(city_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
      "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
      "mean     3.593761   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
      "std      8.596783   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
      "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
      "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
      "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
      "75%      3.647423   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
      "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
      "\n",
      "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
      "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
      "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
      "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
      "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
      "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
      "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
      "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
      "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
      "\n",
      "            LSTAT      target  \n",
      "count  506.000000  506.000000  \n",
      "mean    12.653063   22.532806  \n",
      "std      7.141062    9.197104  \n",
      "min      1.730000    5.000000  \n",
      "25%      6.950000   17.025000  \n",
      "50%     11.360000   21.200000  \n",
      "75%     16.955000   25.000000  \n",
      "max     37.970000   50.000000  \n",
      "Median: \n",
      "21.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "boston_data = datasets.load_boston()\n",
    "boston_df = pd.DataFrame(boston_data['data'], columns=boston_data['feature_names'])\n",
    "boston_df['target'] = boston_data['target']\n",
    "\n",
    "#Chas is a binary variable. ZN and Rad look categorical.  Rest are continuous\n",
    "print(boston_df.describe())\n",
    "print(\"Median: \")\n",
    "print(boston_df['target'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Statistical Analysis and Data Exploration\n",
    "\n",
    "Number of data points (houses)? \n",
    "\n",
    "* 506\n",
    "\n",
    "Number of features?\n",
    "\n",
    "* 12\n",
    "\n",
    "Minimum and maximum housing prices?\n",
    "\n",
    "* 5 and 50\n",
    "\n",
    "Mean and median Boston housing prices?\n",
    "\n",
    "* 22.5 and 21.2\n",
    "\n",
    "Standard deviation?\n",
    "\n",
    "* 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Evaluating Model Performance\n",
    "\n",
    "Which measure of model performance is best to use for predicting Boston housing data and analyzing the errors? \n",
    "Why do you think this measurement most appropriate? Why might the other measurements not be appropriate here?\n",
    "Why is it important to split the Boston housing data into training and testing data? What happens if you do not do this?\n",
    "What does grid search do and why might you want to use it?\n",
    "Why is cross validation useful and why might we use it with grid search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Firstly, this is a regression problem, so that narrows down the applicable measures.  In this case, we want a model the predicts a continous variable and the best measure of both training and testing performance is the devaition between the predicted value and the actual value for each observation.  Because the model can have both negative and positive deviations from the actual labels, we need a measure that accounts for both positive and negative deviations.  The best way to day this is to square the deviations and then average them to get the Mean Squared Error (MSE).\n",
    "\n",
    "There is no special reason to split the Boston housing data into training and testing data, it's just a good practice for getting a sense of how the model will perform in the real world.  Without doing this, you reduce the odds that your model will be general enough to be accurate when making predictions on a new set of features.\n",
    "\n",
    "Grid search is a method of finding a good parameter iteratively.  In this case, we use grid search to find the depth of the regression tree that produces the lowest bias and lowest variance.  In other models, we might use grid search to find the optimal value for the tuning parameter, etc.  It comes with the caveat that you may \"skip\" over the optimal parameter depending on how \"granular\" the grid is or you could end up at a local minimum.  Cross validation is useful for the same reason as previously describe for splitting the data into training and testing data.  In this particular case, K-fold Cross Validation was used as the default setting for sklearn's GridSearchCV() function.  K-fold Cross Validation is useful because we reduce the dependency of the model parameters on how the data was split up into testing and training data for Cross Validation without k-folds. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Analyzing Model Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at all learning curve graphs provided. What is the general trend of training and testing error as training size increases?\n",
    "Look at the learning curves for the decision tree regressor with max depth 1 and 10 (first and last learning curve graphs). When the model is fully trained does it suffer from either high bias/underfitting or high variance/overfitting?\n",
    "Look at the model complexity graph. How do the training and test error relate to increasing model complexity? Based on this relationship, which model (max depth) best generalizes the dataset and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general for most of the depth iterations, the test error decreases as the training sample increases in size.  The training error is generally increasing.  \n",
    "\n",
    "The optimal point in the bias versus variance tradeoff seems to be around max depth =2 (without using k-fold Cross Validation).  At max depth = 1, the model seems to be ok in terms of performance, but the sample size has to increase quite significantly to get a convergence between the test error and the training error.  At max depth = 2, the test error and training error converge quite quickly and there is an overall decrease in the error (i.e. reduction in both variance and bias).  For depth >= 3, signs of overfitting start to occur where you do get a decrease in error but the variance increases with each increase in depth.  At depth =10, the variance as implied by the difference between the test error and traiing error is quite massive.  This suggests the reduction in bias is really the result of overfitting rather than a genuine increase in the predictive power of the model.\n",
    "\n",
    "But, k-fold cross validation suggests something slightly different.  The default setting for the GridSearchCV() function in sklearn is 3 folds.  When using the default setting, it shows that both bias and variance decrease up to max depth=3.  After that point, the mean error stays flat but variance continues to increase.  Both conventional cross validation and k-fold cross validation are saying something very similar (the the optimal max depth is either 2 or 3), but in this case, I would take the k-fold cross validation implied optimal depth =3 as the best model because it should be a more robust validation technique in general but especially because this is not a small data set (i.e. I would trust conventional cross validation if this were a small data set of less than ~50 observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Model Prediction\n",
    "\n",
    "Model makes predicted housing price with detailed model parameters (max depth) reported using grid search. Note due to the small randomization of the code it is recommended to run the program several times to identify the most common/reasonable price/model complexity.\n",
    "Compare prediction to earlier statistics and make a case if you think it is a valid model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is valid and has moderate predictive power.  Our best model has an MSE of 35.06.  This implies that for any given prediction, we could expect an error in the absolute prediction of the house price to be +/- 5.9.  One quick comparison would be to compare the model against a model that simply uses the mean house price of 21.2 for all predictions.  This hypothetical model would be with 9.2 (the standard deviation) 67% of the time.  Comparison against this hypothetical model helps put the error of the model we made in perspective and you can see that it does a fairly decent job.  It's also important to keep in mind that there is no model that will be able to make predictions that are 100% correct because house sales involve an element of randomness.  In addition, there are many more factors that go into house prices that would be impractical to include in the dataset.  For example, there could be one outlier that sold for a price much higher than the predicted amount because it has historical significance.  Even though that could be one case in the data set, outliers can have a big impact on the model parameters and it wouldn't neccessarily be practical to include a dummy variable for historical significance just to try and reduce the error.  In the end, it comes down to, \"what's this model going to be used for?\"!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
